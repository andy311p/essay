One of the weaknesses of state-of-the-art approaches to AES is that they are not well suited for adversarially crafted input of grammatical but incoherent sequences of sentences. Farag et al. [15] tackled this problem by developing a neural model of local coherence that can effectively learn connectedness features between sentences. Using a framework for integrating and jointly training the local coherence model with state-of-the-art AES, they demonstrate its effectiveness on both the AES task and the task of flagging adversarial input.
Pennebaker et al. [14] analyzed a corpus of 50,000 CAEs using features based on 8 groups of function words (pronouns, articles, auxiliary verbs etc.). They then tried to show correlation between the features extracted from the essays, which indicate whether the language is categorical or dynamic, and the grades over students' four years of college. Their findings showed that higher grades were associated with greater article and preposition use, indicating categorical language, while lower grades were associated with greater use of auxiliary verbs, pronouns, adverbs, conjunctions and negations, indicating more dynamic language. The links between categorical and dynamic writing and academic performance hint at the cognitive styles rewarded by education institutions.
Alvero et al. [12] sought to investigate the potential of computational reading as a check up on implicit bias in holistic review. They used a unique corpus of 283,000 application essays submitted to a large, selective, state university to asses the extent to which applicant demographic characteristics can be inferred from their essays. Using Logistic Regression, they were able to predict gender and household income with high levels of accuracy. In another research, Arthurs et al. [13] investigate the bias in methods used to evaluate the quality of vectors. They accomplish this by using a dataset of 800,000 CAEs split into quartiles based on Reported Household Income (RHI). They train sets of word vectors on each of the quartile and test each set on intrinsic evaluation tasks. They reach the conclusion that the evaluation tasks themselves are biased towards the writing of higher income applicants.
Kanojia et al. [11] predicted Computer Science Graduation admission acceptance based on applicants Statement of Purpose. They created a unique dataset by manually collecting 50 Statements of Purpose from Elite Universities (acceptance rate <= 15%). For the classification they used three sets of features – a) Textual Features – Feature values based on the text of the essay, b) Word Embedding Based Features – Features based on average of vector values provided by pre-trained model on Google News Corpora (Word2Vec), c) Similarity Score based and Error based features – Features based on Document Similarity, and other features based on errors in the document. They used conventional Machine Learning algorithms (LR, SVM, RFDT) and simple deep learning approaches (FFNN, MLP) combined with standard K-fold cross validation to deal with the small size of the dataset. 

3.Research goals
Currently, evaluation of CAEs, is a field that received very little attention from the research community. 
The first reason for that is that there is no easy, accessible public dataset. Colleges simply don't publicize applicant's admission data. Research related to CAE datasets was either conducted internally by researchers from a specific institute or by manually collecting CAEs, which resulted in small datasets and limited options when it came to implementing Deep Neural Networks. 
The second reason is that evaluation of CAEs is far more challenging than AES. Although both cases can be viewed as a classification problem where the input is a long text and the output is the classification (accepted/rejected or score range), They are very different in their core. When writing an essay as part of an exam, or a scored test, you have a bound amount of time to write the essay, and usually no additional resources to use such as books or the internet. The written essays represent quite well the writing level of a person. Because of these, written essays have many simple textual features such as grammatical errors, spelling, punctuation, and word counts that can be used in order to effectively score the essays. It’s a whole different story when looking at CAEs. The task itself of writing a CAE is not bound by time. In addition to that, applicants can and are encouraged to use every bit of help available in order to write the best possible essay they can (in the ethical bounds), including several stages of personal revision, grammar and spelling tools, online essay evaluators and many more. The result is that most CAEs are well written with very few spelling and grammatical errors, making prediction based on simple textual features nearly impossible for the majority of essays. 
The work done by Kanojia et al. aimed at checking the influence of different textual features of the essay on the applicant's acceptance. However, their dataset focused on Computer Science Graduates only, which might have caused a bias in the results they got. Also, due to the small size of the dataset, they did not test deep neural networks, which showed better results than conventional ML algorithms on multiple tasks in the past, including AES [16]. This leads me to the conclusion that further improvement is very possible. 
To conclude, the goals of this research are:
a)	Identify simple textual features used in previous AES research that can be used for admission prediction using CAEs.
b)	Define and check the validity of deeper semantic features – such as "interestingness"
c)	Test different Machine Learning algorithms (LR, SVM, RFDT) using the selected features. If a large enough dataset becomes available, consider the implementation of a modern deep neural network architecture such as Bi-LSTM or CNN.



[11] - Is your Statement Purposeless? Predicting Computer Science Graduation Admission Acceptance based on Statement Of Purpose
[12] - AI and Holistic Review: Informing Human Reading in College Admissions
[13] - Whose Truth is the “Ground Truth”? College Admissions Essays and Bias in Word Vector Evaluation Methods
[14] - When Small Words Foretell Academic Success: The Case of College Admissions Essays
[15] - Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input
[16] - A Neural Approach to Automated Essay Scoring
